{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Open Source Models with Ollama\n",
    "\n",
    "This notebook shows how to evaluate open-source models locally with Ollama.\n",
    "\n",
    "We'll use [Nemotron-3-Nano-30B-A3B](https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF) as an example - NVIDIA's 30B parameter model with only 3B active parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Ollama\n",
    "\n",
    "**macOS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o /tmp/Ollama.zip \"https://ollama.com/download/Ollama-darwin.zip\" && \\\n",
    "    unzip -o /tmp/Ollama.zip -d /Applications/ && \\\n",
    "    rm /tmp/Ollama.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linux:** `curl -fsSL https://ollama.com/install.sh | sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Ollama Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!open /Applications/Ollama.app  # macOS\n",
    "# Linux: ollama serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify server is running\n",
    "import time\n",
    "\n",
    "time.sleep(5)\n",
    "!curl -s http://localhost:11434/api/tags | python3 -c \"import sys,json; print('Ollama server is running!' if json.load(sys.stdin) else 'Server not ready')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download GGUF Model\n",
    "\n",
    "Choose a quantization based on your available RAM:\n",
    "\n",
    "| Quantization | Size | RAM Required |\n",
    "|-------------|------|-------|\n",
    "| IQ4_XS | 18.2 GB | 16GB |\n",
    "| Q4_K_M | 24.6 GB | 32GB |\n",
    "| Q8_0 | 33.6 GB | 64GB |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"/tmp/nemotron\", exist_ok=True)\n",
    "\n",
    "# Download IQ4_XS quantization (18.2 GB)\n",
    "!curl -L -o /tmp/nemotron/Nemotron-3-Nano-30B-A3B-IQ4_XS.gguf \\\n",
    "    \"https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF/resolve/main/Nemotron-3-Nano-30B-A3B-IQ4_XS.gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Modelfile and Register with Ollama\n",
    "\n",
    "The Modelfile defines the model's chat template and stop tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = '''FROM /tmp/nemotron/Nemotron-3-Nano-30B-A3B-IQ4_XS.gguf\n",
    "\n",
    "TEMPLATE \"\"\"{{- if .System }}{{ .System }}\n",
    "{{- end }}\n",
    "{{- range .Messages }}\n",
    "{{- if eq .Role \"user\" }}<|start_header_id|>user<|end_header_id|>\n",
    "{{ .Content }}<|eot_id|>\n",
    "{{- else if eq .Role \"assistant\" }}<|start_header_id|>assistant<|end_header_id|>\n",
    "{{ .Content }}<|eot_id|>\n",
    "{{- end }}\n",
    "{{- end }}\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "PARAMETER stop \"<|eot_id|>\"\n",
    "PARAMETER stop \"<|end_of_text|>\"\n",
    "'''\n",
    "\n",
    "with open(\"/tmp/nemotron/Modelfile\", \"w\") as f:\n",
    "    f.write(modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register model with Ollama\n",
    "!ollama create nemotron-nano -f /tmp/nemotron/Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Environment for Inspect AI\n",
    "\n",
    "Set the Ollama base URL so Inspect AI can connect to the local server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OLLAMA_BASE_URL\"] = \"http://localhost:11434/v1\"\n",
    "\n",
    "# Or add to your .env file:\n",
    "# OLLAMA_BASE_URL=http://localhost:11434/v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model into Memory\n",
    "\n",
    "Pre-load the model to avoid timeout during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm up the model (keeps it loaded for 60 minutes)\n",
    "!ollama run nemotron-nano --keepalive 60m \"hi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Run all Open Telco benchmarks using Inspect AI with the `ollama/` model prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"ollama/nemotron-nano\"\n",
    "LIMIT = 1  # samples per benchmark for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/.local/bin/uv run inspect eval \\\n",
    "    src/open_telco/teleqna/teleqna.py \\\n",
    "    src/open_telco/telemath/telemath.py \\\n",
    "    src/open_telco/telelogs/telelogs.py \\\n",
    "    src/open_telco/three_gpp/three_gpp.py \\\n",
    "    --model {MODEL} \\\n",
    "    --limit {LIMIT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/.local/bin/uv run inspect view start --log-dir logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
